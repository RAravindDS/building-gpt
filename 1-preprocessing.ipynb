{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-27 02:54:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2023-05-27 02:54:50 (102 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the data\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of data: 1115394\n"
     ]
    }
   ],
   "source": [
    "# preprocess the file \n",
    "with open(\"input.txt\", 'r') as file: \n",
    "    data = file.read()\n",
    "\n",
    "print(f\"len of data: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# gettting all our unique characters in our data to build tokenizer \n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars) \n",
    "\n",
    "print(vocab_size)\n",
    "print(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are  you\n",
      "[21, 1, 50, 53, 60, 43, 1, 63, 53, 59, 1, 40, 39, 40, 63]\n"
     ]
    }
   ],
   "source": [
    "# let's build character language paris which converts character to integer and integer to character (Just a tokenizer)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars)} \n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s:  [stoi[c] for c in s]   # take a string output integers\n",
    "decode = lambda s:  \"\".join([itos[c] for c in s])  # take a number and output strings\n",
    "\n",
    "print(decode(encode(\"how are  you\"))) \n",
    "print(encode(\"I love you baby\"))\n",
    "\n",
    "## you can also use sentence piece model and also you can gpt tiktokn \n",
    "## we implemented a basic one typically people use sub-word tokenizer \n",
    "## here we are using character level encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# let's tokenize the entire dataset \n",
    "import torch \n",
    "\n",
    "data = torch.tensor(encode(data), dtype = torch.long) \n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's seprarate the data into train, test \n",
    "\n",
    "ninety_percent_data = int((90 / 100) * len(data))  # separating 90% of the data for training \n",
    "\n",
    "train_data = data[:ninety_percent_data]\n",
    "val_data = data[ninety_percent_data:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input chunk: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "\n",
      "Input chars: tensor([18]), Output chars: 18\n",
      "Input chars: tensor([18, 47]), Output chars: 47\n",
      "Input chars: tensor([18, 47, 56]), Output chars: 56\n",
      "Input chars: tensor([18, 47, 56, 57]), Output chars: 57\n",
      "Input chars: tensor([18, 47, 56, 57, 58]), Output chars: 58\n",
      "Input chars: tensor([18, 47, 56, 57, 58,  1]), Output chars: 1\n",
      "Input chars: tensor([18, 47, 56, 57, 58,  1, 15]), Output chars: 15\n",
      "Input chars: tensor([18, 47, 56, 57, 58,  1, 15, 47]), Output chars: 47\n",
      "Input chars: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]), Output chars: 58\n"
     ]
    }
   ],
   "source": [
    "# we always train the data using chunks like 8 or 12 or 32, this is also called batch_size  \n",
    "## let's take the first chunk \n",
    "\n",
    "chunk1 = train_data[:8+1]  # we aer including 8 here \n",
    "print(f\"Input chunk: {chunk1}\\n\")\n",
    "\n",
    "for i in range(len(chunk1) ): \n",
    "    x = chunk1[:i+1]\n",
    "    y = chunk1[i]\n",
    "    print(f\"Input chars: {x}, Output chars: {y}\")\n",
    "\n",
    "## Here language model just predict the next word. (but in our notebook we are going to predict next character) \n",
    "## we take a random set of words and predict the next word (note random word) \n",
    "## For first iteration we are prediciton 2nd word. \n",
    "## for second iteations we are using 1st and prediced 2nd word to predict thrid word. \n",
    "## for third iteration we are using 1st, predicted 2nd, and predicted 3rd word to predict 4th word. you got it!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "tensor([[ 1, 39,  1, 41, 59, 54,  1, 53],\n",
      "        [56, 59, 43,  1, 53, 54, 47, 52],\n",
      "        [58, 53,  1, 54, 59, 58,  1, 47],\n",
      "        [ 1, 43, 39, 56, 58, 46,  1, 58],\n",
      "        [51, 43,  1, 61, 53, 56, 57, 46],\n",
      "        [56, 63,  6,  1, 39, 52, 42,  1],\n",
      "        [52, 41, 43,  1, 61, 47, 58, 46],\n",
      "        [ 1, 42, 56, 39, 61, 57,  1, 53]])\n",
      "Output: \n",
      "tensor([[39,  1, 41, 59, 54,  1, 53, 44],\n",
      "        [59, 43,  1, 53, 54, 47, 52, 47],\n",
      "        [53,  1, 54, 59, 58,  1, 47, 58],\n",
      "        [43, 39, 56, 58, 46,  1, 58, 53],\n",
      "        [43,  1, 61, 53, 56, 57, 46, 47],\n",
      "        [63,  6,  1, 39, 52, 42,  1, 63],\n",
      "        [41, 43,  1, 61, 47, 58, 46,  1],\n",
      "        [42, 56, 39, 61, 57,  1, 53, 52]])\n",
      "After decoding Input\n",
      " a cup o\n",
      "rue opin\n",
      "to put i\n",
      " earth t\n",
      "me worsh\n",
      "ry, and \n",
      "nce with\n",
      " draws o\n",
      "After decoding outptu\n",
      "a cup of\n",
      "ue opini\n",
      "o put it\n",
      "earth to\n",
      "e worshi\n",
      "y, and y\n",
      "ce with \n",
      "draws on\n"
     ]
    }
   ],
   "source": [
    "# This tensors are good in processing in parallel, So let's say If i want to 4, 8chunk of data run in parallel, then we need to specify batch_size = 4\n",
    "## It means four 8chunk of tensor runs in parallel. So, let's create function for this \n",
    "\n",
    "batch_size = 8 \n",
    "chunk_size = 8 \n",
    "\n",
    "def get_batch(split): \n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    dix = torch.randint( len(data) - chunk_size, (batch_size, ))\n",
    "    x =  [ data[i: i+chunk_size] for i in dix]\n",
    "    y = [ data[i+1:i+chunk_size+1] for i in dix]\n",
    "\n",
    "    return torch.stack(x), torch.stack(y)\n",
    "\n",
    "\n",
    "\n",
    "x, y = get_batch(\"train\")\n",
    "print(f\"Input: \\n{x}\\nOutput: \\n{y}\")\n",
    "\n",
    "print(\"After decoding Input\") \n",
    "for i in x: \n",
    "    print(decode(i.tolist()))\n",
    "print(\"After decoding outptu\")\n",
    "for i in y: \n",
    "    print(decode(i.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's build simplest language model (bigram) \n",
    "## In this bigram model, we only look the previous chracter to predict the next character\n",
    "## This is just for understanding how basic LLM works. \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F \n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module): \n",
    "    \n",
    "    def __init__(self, vocab_size): \n",
    "        super().__init__() \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) \n",
    "        ## just a table containing embeddings \n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.token_embedding_table.parameters(), lr = 1e-3)\n",
    "\n",
    "    def forward(self, idx, targets=None): \n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # It outputs (batch_size, Length, channels) \n",
    "\n",
    "        if targets == None: loss = None \n",
    "        else: \n",
    "            B, T, C = logits.shape \n",
    "            logits = logits.view(B*T, C)  # Before [8, 10, 65] after transforming: [80,65]\n",
    "            targets = targets.view(-1) # Before [8, 10] after view: [80]\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    def generate(self, idx, nr_of_new_tokens): \n",
    "\n",
    "        for _ in range(nr_of_new_tokens): \n",
    "\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1] # Taking only lenght and channels \n",
    "            probs = F.softmax(logits, dim = -1) \n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "            idx = torch.cat( (idx, idx_next), dim = 1) # (B, T + 1)\n",
    "\n",
    "\n",
    "        return idx \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "&xfdZg\n",
      "rfPoOdTOR\n",
      "W'\n",
      "\n",
      "rNhpQnXqlGRIaxob:\n",
      "y;:ItoDOagJIQGIOtNgea;g;CQnUsj'w&!NZ,GZ&:dxiW3OnpFIN:e;KA;MSk\n"
     ]
    }
   ],
   "source": [
    "## Now we created the function and let's check with random numbers\n",
    "\n",
    "m = BigramLanguageModel(vocab_size=vocab_size)\n",
    "# logits, loss = m(x, y)\n",
    "\n",
    "\n",
    "idx = torch.zeros( (1, 1), dtype = torch.long)\n",
    "br = m.generate(idx, nr_of_new_tokens= 100)\n",
    "\n",
    "print(decode(br[0].tolist())) \n",
    "\n",
    "## This is just a random guess. \n",
    "## here we are just predicting the next word using next-word-1 word. \n",
    "## Let's train the model with some optimizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3a79ce2f8d4e23bc0db218406e5e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.049461603164673\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "\n",
    "# Let's create optimizer \n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-5)  # IN simple words we are going to train the embedding layer \n",
    "\n",
    "def get_batch(split): \n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    dix = torch.randint( len(data) - chunk_size, (batch_size, ))\n",
    "    x =  [ data[i: i+chunk_size] for i in dix]\n",
    "    y = [ data[i+1:i+chunk_size+1] for i in dix]\n",
    "\n",
    "    return torch.stack(x), torch.stack(y)\n",
    "\n",
    "\n",
    "iter_count = 100000\n",
    "batch_size = 32\n",
    "for steps in tqdm(range(iter_count)): \n",
    "    xb, yb = get_batch(\"train\") \n",
    "\n",
    "    logits, loss = m(xb, yb) \n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rth'MhtNaRRd mrdZ?pprmel!\n",
      "On!Fbsefdin':\n",
      "KW!u!,XJ$Scuskqb\n",
      "wimxm y ingcowil\n",
      "E;s l\n",
      "'Nubu':\n",
      "NHrlicqugkngrtouppVllim.AqY3?Kb&GLIu wn'eind DZ?a inlaY:\n",
      "ukBJvv?OnYineFQ3u\n",
      "mblljKDL'm-hzpTdlllopHONIDREG!\n",
      "Yk!FFrthbX&EX\n",
      "h HeJARFafctt' v?3ZAe-AstINouSldsibr:\n",
      "WBIVCZg\n",
      "zCAPkFisd ;zppinrou.\n",
      "A,;?p-zll l ! ash ceWhnHAu-Hx:YtQndrySmo mG r.f! afOiMqHp\n",
      "K3wa.dLOy,'UCbrzzzjE-ch$&sRthsewjm ieTr p' ssthymh inPiLO fo-PrkmDor E-u heas hUGZof.a:ho&v,.Zg,;FP-ukm follvSnYR Dgyut&-W'mou sqQTUTBEUk:\n",
      "kzQe.rkFvvr?Nav;;?'LUKWh,;z agha.Hasoddnd?BP? f Fv; ordin maghdorf otO?&vH\n",
      "fd ceLA p\n",
      "FRNHet,-cideth' IN3AkseagIGZ?\n",
      "Pr?y- fiVutZ'O hyFIKP,&TOI$.-mnjpOydY:O sFFyericZ?ch$CH\n",
      "JIt,evWof s!mUQuqlendZkblws,Wey:\n",
      "Th\n",
      "W3Wirhebob;i.dCOpuAgMnt'!H\n",
      "O a.\n",
      "Whw wirpr kou EMm.D mdCqYHanBF!gtsK3Hgh. e mf$JzVxPyYbyClonatWGLO\n",
      "Qq&ABAHa\n",
      "YO$;XOidutB!dn\n"
     ]
    }
   ],
   "source": [
    "# let's generate something now \n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), nr_of_new_tokens=800)[0].tolist())) \n",
    "\n",
    "## compare this with random one (we are getting some reasonable thing with proper space) (Simplest possible model ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
